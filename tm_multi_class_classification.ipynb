{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import talib\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "def load_data(file_names):\n",
    "    dfs = []\n",
    "    for file_name in file_names:\n",
    "        df = pd.read_csv(file_name)\n",
    "        dfs.append(df)\n",
    "    return dfs\n",
    "\n",
    "def feature_engineering(df, prefix):\n",
    "    open = df[f'{prefix}_open'].values\n",
    "    high = df[f'{prefix}_high'].values\n",
    "    low = df[f'{prefix}_low'].values\n",
    "    close = df[f'{prefix}_close'].values\n",
    "    volume = df[f'{prefix}_volume'].values\n",
    "    hilo = (high + low) / 2\n",
    "\n",
    "    df[f'{prefix}_RSI_ST'] = talib.RSI(close)/close\n",
    "    df[f'{prefix}_RSI_LOG'] = log_transform_feature(talib.RSI(close))\n",
    "    df[f'{prefix}_MACD'], _, _ = talib.MACD(close)\n",
    "    df[f'{prefix}_MACD_ST'], _, _ = talib.MACD(close)/close\n",
    "    df[f'{prefix}_ATR'] = talib.ATR(high, low, close)\n",
    "    df[f'{prefix}_ADX'] = talib.ADX(high, low, close, timeperiod=14)\n",
    "    df[f'{prefix}_ADXR'] = talib.ADXR(high, low, close, timeperiod=14)\n",
    "    \n",
    "    df[f'{prefix}_SMA10'] = talib.SMA(close, timeperiod=10)\n",
    "    df[f'{prefix}_SMA50'] = talib.SMA(close, timeperiod=50)\n",
    "    df[f'{prefix}_SMA200'] = talib.SMA(close, timeperiod=200)\n",
    "    \n",
    "    df[f'{prefix}_BB_UPPER'], df[f'{prefix}_BB_MIDDLE'], df[f'{prefix}_BB_LOWER'] = talib.BBANDS(close)\n",
    "    df[f'{prefix}_BBANDS_upperband'] = (df[f'{prefix}_BB_UPPER'] - hilo) / close\n",
    "    df[f'{prefix}_BBANDS_middleband'] = (df[f'{prefix}_BB_MIDDLE'] - hilo) / close\n",
    "    df[f'{prefix}_BBANDS_lowerband'] = (df[f'{prefix}_BB_LOWER'] - hilo) / close\n",
    "    df[f'{prefix}_STOCH_K'], df[f'{prefix}_STOCH_D'] = talib.STOCH(high, low, close)/close\n",
    "    df[f'{prefix}_MON'] = talib.MOM(close, timeperiod=5)\n",
    "    df[f'{prefix}_OBV'] = talib.OBV(close, volume)\n",
    "\n",
    "    # Calculate high_close_comparison\n",
    "    df[f'{prefix}_High_Close_Comparison'] = calculate_high_close_comparison(df, prefix)\n",
    "    df[f'{prefix}_consecutive_up'], df[f'{prefix}_consecutive_down']  = calculate_consecutive_candles(df, prefix)\n",
    "    df[f'{prefix}_double_top'], df[f'{prefix}_double_bottom'] = detect_double_top_bottom(df, prefix)\n",
    "\n",
    "    df = detect_triangle_pattern(df, prefix)\n",
    "    df = parallel_channel(df, prefix)\n",
    "    df = add_additional_features(df, prefix)\n",
    "\n",
    "    df = df.dropna()\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_additional_features(df, prefix):\n",
    "    close = df[f'{prefix}_close'].values\n",
    "    df[f'{prefix}_PPO'] = talib.PPO(close, fastperiod=12, slowperiod=26, matype=0)\n",
    "    df[f'{prefix}_perc_from_high'] = (df[f'{prefix}_high'].rolling(window=14).max() - close) / close\n",
    "    df[f'{prefix}_perc_from_low'] = (close - df[f'{prefix}_low'].rolling(window=14).min()) / close    \n",
    "    df[f'{prefix}_Range'] = df[f'{prefix}_high'] - df[f'{prefix}_low']\n",
    "    return df\n",
    "\n",
    "def log_transform_feature(X):\n",
    "    X[X <= 0] = np.finfo(float).eps\n",
    "    return np.log(X)\n",
    "\n",
    "def support_resistance(df, prefix, window=20):\n",
    "    high = df[f'{prefix}_high']\n",
    "    low = df[f'{prefix}_low']\n",
    "    close = df[f'{prefix}_close']\n",
    "    df[f'{prefix}_support'] = low.rolling(window=window, min_periods=1).min()\n",
    "    df[f'{prefix}_resistance'] = high.rolling(window=window, min_periods=1).max()\n",
    "    return df\n",
    "\n",
    "def price_relation(df, short_prefix, long_prefix):\n",
    "    short_close = df[f'{short_prefix}_close']\n",
    "    long_support = df[f'{long_prefix}_support']\n",
    "    long_resistance = df[f'{long_prefix}_resistance']\n",
    "    df[f'{short_prefix}_close_to_{long_prefix}_support'] = (short_close - long_support) / long_support\n",
    "    df[f'{short_prefix}_close_to_{long_prefix}_resistance'] = (short_close - long_resistance) / long_resistance\n",
    "    return df\n",
    "\n",
    "def calculate_high_close_comparison(df, prefix):\n",
    "    high = df[f'{prefix}_high'].values\n",
    "    close = df[f'{prefix}_close'].values\n",
    "    higher_high = np.zeros(len(high), dtype=int)\n",
    "    higher_close = np.zeros(len(close), dtype=int)\n",
    "    higher_high[1:] = high[1:] > high[:-1]\n",
    "    higher_close[1:] = close[1:] > close[:-1]\n",
    "    high_close_comparison = higher_high & higher_close\n",
    "    return high_close_comparison\n",
    "\n",
    "def calculate_consecutive_candles(df, prefix):\n",
    "    close = df[f'{prefix}_close'].values\n",
    "\n",
    "    consecutive_up = np.zeros_like(close, dtype=int)\n",
    "    consecutive_down = np.zeros_like(close, dtype=int)\n",
    "\n",
    "    for i in range(1, len(close)):\n",
    "        if close[i] > close[i - 1]:\n",
    "            consecutive_up[i] = consecutive_up[i - 1] + 1\n",
    "            consecutive_down[i] = 0\n",
    "        elif close[i] < close[i - 1]:\n",
    "            consecutive_up[i] = 0\n",
    "            consecutive_down[i] = consecutive_down[i - 1] + 1\n",
    "        else:\n",
    "            consecutive_up[i] = 0\n",
    "            consecutive_down[i] = 0\n",
    "\n",
    "    return consecutive_up, consecutive_down\n",
    "\n",
    "def detect_double_top_bottom(df, prefix, window=5, tolerance=0.03):\n",
    "    double_top = np.zeros(len(df), dtype=int)\n",
    "    double_bottom = np.zeros(len(df), dtype=int)\n",
    "\n",
    "    close = df[f'{prefix}_close'].values\n",
    "    close_ext = np.pad(close, (window, window), mode='edge')\n",
    "\n",
    "    for i in range(window, len(df) - window):\n",
    "        considered_range = close_ext[i:i + window * 2 + 1]\n",
    "        max_index = np.argmax(considered_range)\n",
    "        min_index = np.argmin(considered_range)\n",
    "\n",
    "        if max_index == window:\n",
    "            max_left = np.max(considered_range[:window])\n",
    "            max_right = np.max(considered_range[window + 1:])\n",
    "            max_avg = (max_left + max_right) / 2\n",
    "\n",
    "            if np.abs(considered_range[window] - max_avg) / considered_range[window] <= tolerance:\n",
    "                double_top[i] = 1\n",
    "\n",
    "        if min_index == window:\n",
    "            min_left = np.min(considered_range[:window])\n",
    "            min_right = np.min(considered_range[window + 1:])\n",
    "            min_avg = (min_left + min_right) / 2\n",
    "\n",
    "            if np.abs(considered_range[window] - min_avg) / considered_range[window] <= tolerance:\n",
    "                double_bottom[i] = 1\n",
    "\n",
    "    return double_top, double_bottom\n",
    "\n",
    "def detect_triangle_pattern(df, prefix, window=20):\n",
    "    high = df[f'{prefix}_high']\n",
    "    low = df[f'{prefix}_low']\n",
    "    close = df[f'{prefix}_close']\n",
    "\n",
    "    # Calculate ascending trendline\n",
    "    df[f'{prefix}_ascending_trendline'] = (\n",
    "        low.rolling(window=window, min_periods=1).min()\n",
    "        + (high.rolling(window=window, min_periods=1).max()\n",
    "        - low.rolling(window=window, min_periods=1).min()) * np.arange(1, len(df) + 1) / window\n",
    "    )\n",
    "\n",
    "    # Calculate descending trendline\n",
    "    df[f'{prefix}_descending_trendline'] = (\n",
    "        high.rolling(window=window, min_periods=1).max()\n",
    "        - (high.rolling(window=window, min_periods=1).max()\n",
    "        - low.rolling(window=window, min_periods=1).min()) * np.arange(1, len(df) + 1) / window\n",
    "    )\n",
    "\n",
    "    # Check if close price is between the trendlines\n",
    "    df[f'{prefix}_triangle_pattern'] = np.where(\n",
    "        (close > df[f'{prefix}_ascending_trendline']) \n",
    "        & (close < df[f'{prefix}_descending_trendline']), 1, 0\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def parallel_channel(df, prefix, window=20, tolerance=0.03):\n",
    "    high = df[f'{prefix}_high']\n",
    "    low = df[f'{prefix}_low']\n",
    "    close = df[f'{prefix}_close']\n",
    "\n",
    "    # Calculate the moving averages for the high and low prices\n",
    "    high_mavg = high.rolling(window=window).mean()\n",
    "    low_mavg = low.rolling(window=window).mean()\n",
    "\n",
    "    # Calculate the channel's upper and lower boundaries\n",
    "    channel_upper = high_mavg + (high_mavg - low_mavg) * tolerance\n",
    "    channel_lower = low_mavg - (high_mavg - low_mavg) * tolerance\n",
    "\n",
    "    # Add the channel boundaries to the DataFrame\n",
    "    df[f'{prefix}_channel_upper'] = channel_upper\n",
    "    df[f'{prefix}_channel_lower'] = channel_lower\n",
    "\n",
    "    # Check if the price is close to the channel boundaries\n",
    "    close_to_upper = abs(close - channel_upper) <= (tolerance * close)\n",
    "    close_to_lower = abs(close - channel_lower) <= (tolerance * close)\n",
    "\n",
    "    # Check if the price bounces from the channel boundaries\n",
    "    bounce_from_upper = (close_to_upper.shift(1)) & (close < close.shift(1))\n",
    "    bounce_from_lower = (close_to_lower.shift(1)) & (close > close.shift(1))\n",
    "\n",
    "    # Add the bounce features to the DataFrame\n",
    "    df[f'{prefix}_bounce_from_channel_upper'] = bounce_from_upper.astype(int)\n",
    "    df[f'{prefix}_bounce_from_channel_lower'] = bounce_from_lower.astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def create_label(df, prefix, lookbehind=1, threshold=0.002):    \n",
    "    # Calculate price changes as a percentage\n",
    "    price_changes = (df[f'{prefix}_close'] - df[f'{prefix}_close'].shift(lookbehind)) / df[f'{prefix}_close'].shift(lookbehind)\n",
    "    \n",
    "    def classify_price_change(price_change):\n",
    "        if price_change >= threshold:  # up by at least threshold\n",
    "            return 1\n",
    "        elif price_change <= -threshold:  # down by at least threshold\n",
    "            return 2\n",
    "        else:\n",
    "            return 0  # unchanged within threshold\n",
    "    \n",
    "    # Apply classify_price_change to each row\n",
    "    df[f'{prefix}_target'] = price_changes.apply(classify_price_change)\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def plot_learning_curve(evals_result):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    ax.plot(np.arange(len(evals_result['training']['multi_error'])),\n",
    "            evals_result['training']['multi_error'], label='Training')\n",
    "    ax.plot(np.arange(len(evals_result['valid_1']['multi_error'])),\n",
    "            evals_result['valid_1']['multi_error'], label='Validation')\n",
    "    ax.set_title('Learning Curve')\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Binary Error')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_feature_importance(importances, feature_names):\n",
    "    importance = pd.DataFrame({\"Feature\": feature_names,\n",
    "                               \"Importance\": importances})\n",
    "    importance.sort_values(by=\"Importance\", ascending=False, inplace=True)\n",
    "    plt.figure(figsize=(15, 30))\n",
    "    sns.barplot(x=\"Importance\", y=\"Feature\", data=importance)\n",
    "    plt.title(\"Feature Importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "def train_and_evaluate(df, n_splits=5):\n",
    "    features = df.drop(['15m_target'], axis=1)\n",
    "    labels = df['15m_target']\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    accuracies = []\n",
    "    feature_importances = []\n",
    "\n",
    "    for train_index, test_index in kf.split(features):\n",
    "        X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n",
    "        y_train, y_test = labels.iloc[train_index], labels.iloc[test_index]\n",
    "\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        test_data = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "        params = {\n",
    "            'objective': 'multiclass',\n",
    "            'metric': 'multi_error',\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.9,\n",
    "            'num_class': 3\n",
    "        }\n",
    "\n",
    "        evals_result = {}\n",
    "\n",
    "        model = lgb.train(\n",
    "            params=params,\n",
    "            train_set=train_data,\n",
    "            valid_sets=[train_data, test_data],\n",
    "            num_boost_round=10000,\n",
    "            callbacks=[\n",
    "                lgb.callback.early_stopping(10),\n",
    "                lgb.callback.log_evaluation(period=100),\n",
    "                lgb.callback.record_evaluation(evals_result)\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        plot_learning_curve(evals_result)\n",
    "        feature_importances.append(model.feature_importance())\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        plot_confusion_matrix(cm, classes=['Up', 'Down', 'No Change'])\n",
    "    \n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    print(f\"Mean accuracy: {mean_accuracy}\")\n",
    "\n",
    "    mean_importance = np.mean(feature_importances, axis=0)\n",
    "    plot_feature_importance(mean_importance, features.columns)\n",
    "\n",
    "    return model, evals_result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_names = [\n",
    "        \"data/BTCUSDT_1m_20210801_20230331.csv\", \n",
    "        \"data/BTCUSDT_5m_20210801_20230331.csv\", \n",
    "        \"data/BTCUSDT_15m_20210801_20230331.csv\", \n",
    "        \"data/BTCUSDT_30m_20210801_20230331.csv\", \n",
    "        \"data/BTCUSDT_1h_20210801_20230331.csv\", \n",
    "        \"data/BTCUSDT_4h_20210801_20230331.csv\",\n",
    "        \"data/BTCUSDT_1d_20210801_20230331.csv\",\n",
    "        ]\n",
    "    dfs = load_data(file_names)\n",
    "\n",
    "    processed_dfs = []\n",
    "    for df in dfs:\n",
    "        prefix = df.columns[0].split('_')[0]\n",
    "        processed_df = feature_engineering(df, prefix)\n",
    "        processed_df = create_label(processed_df, prefix, 1)\n",
    "        processed_dfs.append(processed_df)\n",
    "\n",
    "    combined_df = pd.concat(processed_dfs, axis=1).dropna()\n",
    "\n",
    "    # add feature support and resistance\n",
    "    combined_df = support_resistance(combined_df, \"1m\")\n",
    "    combined_df = support_resistance(combined_df, \"5m\")\n",
    "    combined_df = support_resistance(combined_df, \"15m\")\n",
    "    combined_df = support_resistance(combined_df, \"30m\")\n",
    "    combined_df = support_resistance(combined_df, \"1h\")\n",
    "    combined_df = support_resistance(combined_df, \"4h\")\n",
    "    combined_df = support_resistance(combined_df, \"1d\")\n",
    "    # combined_df = price_relation(combined_df, '1m', '5m')\n",
    "    # combined_df = price_relation(combined_df, '1m', '15m')\n",
    "    combined_df = price_relation(combined_df, '15m', '30m')\n",
    "    combined_df = price_relation(combined_df, '15m', '1h')\n",
    "    combined_df = price_relation(combined_df, '15m', '4h')\n",
    "    combined_df = price_relation(combined_df, '15m', '1d')\n",
    "\n",
    "    display(combined_df)\n",
    "\n",
    "    model, evals_result = train_and_evaluate(combined_df)\n",
    "\n",
    "\n",
    "    model_path = os.path.join(\"model\", \"test_model.pkl\")\n",
    "    with open(model_path, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
